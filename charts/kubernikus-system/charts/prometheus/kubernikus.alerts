groups:
- name: kubernikus.alerts
  rules:
  - alert: KubernikusKlusterStuck
    expr: kubernikus_kluster_status_phase{phase!="Running"} == 1
    for: 1h
    labels:
      tier: kks
      service: k8s
      severity: warning
      context: kluster
      meta: "{{ $labels.kluster_id }} stuck in {{ $labels.phase }}"
    annotations:
      description: Kluster {{ $labels.kluster_id }} is stuck in {{ $labels.phase }} for 1h
      summary: Kluster stuck in phase {{ $labels.phase }}

  - alert: KubernikusAPIDown
    expr: count by (instance) (probe_success{kubernetes_name="kubernikus-api"} != 1) >= count by (instance) (probe_success{kubernetes_name="kubernikus-api"} == 1)
    for: 5m
    labels:
      tier: kks
      service: kubernikus
      severity: critical
      context: availability
    annotations:
      description: "{{ $labels.instance }} is unavailable"
      summary: "{{ $labels.instance }} is unavailable"
  
  - alert: KubernikusKlusterUnavailable
    expr: (probe_success{kubernetes_namespace="kubernikus"} != 1) / on (kubernetes_name) label_replace(kubernikus_kluster_status_phase{phase="Running"} == 1, "kubernetes_name", "$1", "kluster_id", "(.*)")
    for: 20m
    labels:
      tier: kks
      service: kubernikus
      severity: warning
      context: kluster
      meta: "{{ $labels.kubernetes_name }} is unavailable"
    annotations:
      description: "{{ $labels.kubernetes_name }} is unavailable since 10m"
      summary: "{{ $labels.kubernetes_name }} is unavailable"

- name: operator.alerts
  rules:
  - alert: KubernikusOperatorGoroutineLeak
    expr: sum(rate(go_goroutines{app="kubernikus",type="operator"}[5m])) by (app,type) > sum(avg_over_time(go_goroutines{app="kubernikus",type="operator"}[12h] offset 12h)) by (app,type)
    for: 10m
    labels:
      tier: kks
      service: operator
      severity: warning
      context: operator
    annotations:
      description: High number of goroutines in kubernikus operator
      summary: Goroutine leak in kubernikus operator

  - alert: KubernikusLaunchOperationErrorSpike
    expr: sum(irate(kubernikus_launch_failed_operation_total[5m])) by (method) > sum(avg_over_time(kubernikus_launch_failed_operation_total[12h] offset 12h)) by (method)
    for: 10m
    labels:
      tier: kks
      service: launch
      severity: warning
      context: operator
    annotations:
      description: Unusually high amount of failed launchctl operations
      summary: Unusually many launchctl failures

  - alert: KubernikusLaunchHanging
    expr: sum(kubernikus_launch_operation_total) == 0
    for: 15m
    labels:
      tier: kks
      service: launchctl
      severity: critical
      context: operator
    annotations:
      description: Launchctl operations dropped to 0. The operator might be hanging.
      summary: Launchctl operations dropped to 0

  - alert: KubernikusRouteGcOperationErrorSpike
    expr: sum(irate(kubernikus_routegc_failed_operation_total[5m])) by (method) > sum(avg_over_time(kubernikus_routegc_failed_operation_total[12h] offset 12h)) by (method)
    for: 10m
    labels:
      tier: kks
      service: routegc
      severity: warning
      context: operator
    annotations:
      description: Unusually high amount of failed routegc operations
      summary:  Unusually many routegc failures

  - alert: KubernikusDeorbiterHanging
    expr: sum(kubernikus_deorbit_operation_total) == 0
    for: 10m
    labels:
      tier: kks
      service: deorbit
      severity: critical
      context: operator
    annotations:
      description: Deorbiter operations dropped to 0. The operator might be hanging.
      summary: Deorbiter operations dropped to 0

  - alert: KubernikusHammertime
    expr: kubernikus_hammertime_status == 1
    for: 10m
    labels:
      tier: kks
      service: hammertime
      severity: warning
      context: operator
      meta: "{{ $labels.kluster }} is having Hammertime"
    annotations:
      description: Hammertime for kluster {{ $labels.kluster }}. Controller manager scaled down because no node posted a status update recently.
      summary: Hammertime for kluster {{ $labels.kluster }}

  - alert: KubernikusMigrationErrors
    expr: increase(kubernikus_migration_errors_total[6m]) > 1
    for: 10m
    labels:
      tier: kks
      service: migration
      severity: warning
      context: operator
      meta: "Migration(s) for kluster {{ $labels.kluster }} failing"
    annotations:
      description: The kluster {{ $labels.kluster }} is generating errors while trying to apply pending migrations.
      summary: Migration errors for kluster {{ $labels.kluster }}

  - alert: KubernikusEtcdBackupFailed
    expr: (increase(etcdbr_snapshot_latest_timestamp{kind="Full"}[3h]) == 0) / on (release) label_replace(kubernikus_kluster_status_phase{phase="Running"} == 1, "release", "$1", "kluster_id", "(.*)")
    for: 5m
    labels:
      tier: kks
      service: kubernikus
      severity: warning
      context: kluster
      meta: "Etcd backup for kluster {{ $labels.release }} failing"
    annotations:
      description: Backup of etcd is failing for kluster {{ $labels.release }}. There is no full backup created for at least 3h.
      summary: Etcd backup error for kluster {{ $labels.release }}